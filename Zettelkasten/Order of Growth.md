---
tags: algorithms
---
In the [[Insertion Sort]] analysis, we actualy had much more detail than we actually need, we defined constants $c_{k}$ to represent the costs of the steps of algorithms, the insertion sort algorithm, rather simple, becomes very unwiedly very quickly, thus we defined some [[Functions|functions]] in terms of the input size $n$, the best case being a function of the form $an +b$ and the worst case being [[Quadratic Function]]s $an^{2}+bn+c$, in fact, this is more detail than we need, because when $n$ tends to infinity, values other than the degree of the leading monomial in the overall [[Polynomials|polynomial]] become much smaller. Since constant factors are less significant than the **rate of growth** in determining the computational efficiency for larger inputs, we ignore it also.
To represent this we use a "theta notation" i.e $\Theta(n)$ where $n$ is the size of the input, in this context, $\Theta$ means "*roughly proportional to $n$ when $n$ is large*". We usually consider a algorithm to be more efficient than other if the worst case running time has a lower order of growth. Due to constant factors and hidden lower order terms, a algorithm with a higher order of growth might run faster on smaller inputs, but on large enough inputs, a algorithm that takes $\Theta(n^{2})$ will run faster than a algorithm of $\Theta(n^3)$ regardless of the hidden constants and terms, there is always some number $n_{0}$ such that for all input sizes $n \ge n_{0}$, the $\Theta(n^{2})$ beats the $\Theta(n^{3})$ one.